---
title: "Machine learning in R part 1: regression"
author: "Jeff Oliver"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: default
  pdf_document:
    latex_engine: xelatex
urlcolor: blue
---

[INTRODUCTORY SENTENCE]

**Something about machine learning, introduction to the concept**

#### Learning objectives
1. Explain the difference between machine learning and inferential statistics
2. Load tabular data into R
3. Build a predictive model and evaluate model performance
4. Split data into training and testing sets
5. Apply iteration to model evaluation and model selection

## [DESCRIPTION OR MOTIVATION; 2-4 sentences that would be used for an announcement]

***

## Getting started

### Install additional R packages

There are _two_ additional R packages that will need to be installed:

+ dplyr
+ randomforest

To install these, run:

```{r install-libraries, eval = FALSE}
install.packages("dplyr")
install.packages("randomforest")
```

### Workspace organization

First we need to setup our development environment. Open RStudio and create a 
new project via:

+ File > New Project...
+ Select 'New Directory'
+ For the Project Type select 'New Project'
+ For Directory name, call it something like "ml-regression" (without the quotes)
+ For the subdirectory, select somewhere you will remember (like "My Documents" 
or "Desktop")

We need to create two folders: 'data' will store the data we will be analyzing, 
and 'output' will store the results of our analyses. In the RStudio console:

```{r workspace-setup, eval = FALSE}
dir.create(path = "data")
dir.create(path = "output")
```

It is good practice to keep input (i.e. the data) and output separate. 
Furthermore, any work that ends up in the **output** folder should be completely
disposable. That is, the combination of data and the code we write should allow 
us (or anyone else, for that matter) to reproduce any output.

### Example data

Explain where the data come from

save it in the 'data' folder that you created in the step above.

***

## Machine learning in a nutshell

In this lesson, we are going to run through an relatively minimal example of 
machine learning. The term "machine learning" gets thrown around a lot, often 
with little explanation, so we will start with a _very_ brief explanation. The 
big difference between "traditional" statistics and machine learning is the 
goal of each approach (aside: the modifier "traditional" is in no way meant to 
imply a lesser status or utility of statistics - I just could not come up with 
a better term). That is, the statistics we learned in class are generally 
focused on making inferences about how the world works, i.e. how does $X$ 
affect $Y$? In contrast, machine learning is less concerned with the details of 
how $X$ and $Y$ are related, but rather focused on using $X$ to make accurate 
predictions of $Y$. If we consider this in a linear regression framework, 
statistics cares about getting accurate values for the beta hats 
($\hat{\beta}$) on the right-hand side of the equation, while machine learning 
cares about being able to make accurate predictions for the Y hats ($\hat{Y}$) 
on the left-hand side of the equation:

$$
\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X_1 
$$

***

```{r dev-block}
# Place for experimentation; replace later
# Merging Taylor Swift data

library(dplyr)
library(randomForest)

# Chart data for Beyonce (& TS) available at 
# https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-09-29
# How do we get song characteristics? Might look at how it was done for TS:
# https://github.com/wjakethompson/taylor

# Song characteristics
# https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-10-17/taylor_all_songs.csv
# Billboard performance
# https://github.com/scharfsteina/BigDataFinalProject/blob/main/data/billboard.csv


## TOPIC FIVE (data wrangling)

songs <- read.csv(file = "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-10-17/taylor_all_songs.csv")
scores <- read.csv(file = "https://raw.githubusercontent.com/scharfsteina/BigDataFinalProject/main/data/billboard.csv")
colnames(scores)[1] <- "id"

# songs <- read.csv(file = "~/Desktop/taylor_all_songs.csv")
# scores <- read.csv(file = "~/Desktop/billboard.csv")

songs$track_name <- tolower(songs$track_name)

combined <- scores %>%
  left_join(songs, by = join_by(song == track_name))

# We want to predict peak_position
# Predictors (aka "features")
# danceability energy loudness speechiness acousticness instrumentalness liveness valence tempo explicit mode_name

model_data <- combined %>%
  select(peak_position, danceability, energy, loudness, speechiness, 
         acousticness, instrumentalness, liveness, valence, tempo, explicit, 
         mode_name) %>% 
  na.omit()

# Run a linear model with all predictors
linear_model <- lm(peak_position ~ ., data = model_data)

# Run a random forest model with all predictors
rf_model <- randomForest(peak_position ~ ., data = model_data)

## TOPIC TWO

# Comparing model performance

# Predict song's peak position based on linear model
linear_prediction <- predict(linear_model, newdata = model_data)

# Predict song's peak position based on random forest model
rf_prediction <- predict(rf_model, newdata = model_data)

# Calculate RMSE for each model: square root of the mean squared error
linear_mse <- mean((model_data$peak_position - linear_prediction)^2)
linear_rmse <- sqrt(linear_mse)

rf_mse <- mean((model_data$peak_position - rf_prediction)^2)
rf_rmse <- sqrt(rf_mse)

cat("Linear model RMSE: ", linear_rmse, "\n")
cat("Random forest RMSE: ", rf_rmse)

## TOPIC THREE

# Splitting data into testing and training

# Create fold vector
fold <- rep_len(x = 1:5, length.out = nrow(model_data))

# One test/train
training <- model_data %>%
  filter(fold != 1)
testing <- model_data %>%
  filter(fold == 1)

# Use training data only to build & evaluate model

# Run a linear model with all predictors
linear_model <- lm(peak_position ~ ., data = training)

# Run a random forest model with all predictors
rf_model <- randomForest(peak_position ~ ., data = training)

# Predict song's peak position based on linear model for TESTING data
linear_prediction <- predict(linear_model, newdata = testing)

# Predict song's peak position based on random forest model for TESTING data
rf_prediction <- predict(rf_model, newdata = testing)

# Calculate RMSE for each model: square root of the mean squared error
linear_mse <- mean((testing$peak_position - linear_prediction)^2)
linear_rmse <- sqrt(linear_mse)

rf_mse <- mean((testing$peak_position - rf_prediction)^2)
rf_rmse <- sqrt(rf_mse)

# cat("Linear model RMSE: ", linear_rmse, "\n")
# cat("Random forest RMSE: ", rf_rmse)

# Note both got worse, but RF got considerably worse (almost double RMSE)

# Now need to do this for each of the folds
# Data frame to hold results
ml_results <- data.frame(fold = 1:5,
                         linear_rmse = NA,
                         rf_rmse = NA)
for (f in 1:5) {
  # One test/train
  training <- model_data %>%
    filter(fold != f)
  testing <- model_data %>%
    filter(fold == f)
  
  # Use training data only to build & evaluate model
  
  # Run a linear model with all predictors
  linear_model <- lm(peak_position ~ ., data = training)
  
  # Run a random forest model with all predictors
  rf_model <- randomForest(peak_position ~ ., data = training)
  
  # Predict song's peak position based on linear model for TESTING data
  linear_prediction <- predict(linear_model, newdata = testing)
  
  # Predict song's peak position based on random forest model for TESTING data
  rf_prediction <- predict(rf_model, newdata = testing)
  
  # Calculate RMSE for each model: square root of the mean squared error
  linear_mse <- mean((testing$peak_position - linear_prediction)^2)
  linear_rmse <- sqrt(linear_mse)
  
  rf_mse <- mean((testing$peak_position - rf_prediction)^2)
  rf_rmse <- sqrt(rf_mse)
  
  ml_results$linear_rmse[f] <- linear_rmse
  ml_results$rf_rmse[f] <- rf_rmse
}

ml_results

# Calculate means for each
linear_mean <- mean(ml_results$linear_rmse)
rf_mean <- mean(ml_results$rf_rmse)

cat("Linear model mean RMSE: ", linear_mean, "\n")
cat("Random forest mean RMSE: ", rf_mean, "\n")

# Make a plot?


```

## [TOPIC ONE]

Running the two models

Vanilla linear regression
Regression trees

***

## [TOPIC TWO]

Comparing the two models via RMSE (no training/testing split)

***

## [TOPIC THREE]

Training & testing split, a little bit why, mostly how
Comparing the two models via RMSE means

***

## [TOPIC FOUR]

Next steps

1. Variable (feature) selection
2. Poisson regression instead of vanilla linear regression
3. Poisson model for random forest

***

## [TOPIC FIVE]

Data wrangling code (read from tidytuesday sources & reduction to only those 
variables of interest)

***

## Additional resources

+ An extremely useful resource is [An Introduction to Statistical Learning with Applications in R](https://arizona-primo.hosted.exlibrisgroup.com/permalink/f/evot53/01UA_ALMA51605092790003843), 
by Gareth James and colleagues. This book provides an accessible introduction 
to several machine learning approaches, including regression and classification
techniques, along with _a lot_ of worked examples and R code.
+ [resource two](url-two)
+ A [PDF version](https://jcoliver.github.io/learn-r/lesson-name.pdf) of this lesson

***

<a href="index.html">Back to learn-r main page</a>
  
Questions?  e-mail me at <a href="mailto:jcoliver@arizona.edu">jcoliver@arizona.edu</a>.