---
title: "Machine learning in R part 1: regression"
author: "Jeff Oliver"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: default
  pdf_document:
    latex_engine: xelatex
urlcolor: blue
---

[INTRODUCTORY SENTENCE]

**Something about machine learning, introduction to the concept**

#### Learning objectives
1. Explain the difference between machine learning and inferential statistics
2. Load tabular data into R
3. Build a predictive model and evaluate model performance
4. Split data into training and testing sets
5. Apply iteration to model evaluation and model selection

## [DESCRIPTION OR MOTIVATION; 2-4 sentences that would be used for an announcement]



***

## Getting started

+ Library installation (if necessary)
+ Project setup
+ Download data (if necessary)

***

## [INTRODUCTION]

Machine learning is different enough that it warrants a little introduction, 
largely just the y-hat vs. beta-hat nature of machine learning vs. statistics.

***

```{r dev-block}
# Place for experimentation; replace later
# Merging Taylor Swift data

library(dplyr)
library(randomForest)

# Chart data for Beyonce (& TS) available at 
# https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-09-29
# How do we get song characteristics? Might look at how it was done for TS:
# https://github.com/wjakethompson/taylor

# Song characteristics
# https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-10-17/taylor_all_songs.csv
# Billboard performance
# https://github.com/scharfsteina/BigDataFinalProject/blob/main/data/billboard.csv


## TOPIC ONE

songs <- read.csv(file = "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-10-17/taylor_all_songs.csv")
scores <- read.csv(file = "https://raw.githubusercontent.com/scharfsteina/BigDataFinalProject/main/data/billboard.csv")
colnames(scores)[1] <- "id"

# songs <- read.csv(file = "~/Desktop/taylor_all_songs.csv")
# scores <- read.csv(file = "~/Desktop/billboard.csv")

songs$track_name <- tolower(songs$track_name)

combined <- scores %>%
  left_join(songs, by = join_by(song == track_name))

# We want to predict peak_position
# Predictors (aka "features")
# danceability energy loudness speechiness acousticness instrumentalness liveness valence tempo explicit mode_name

model_data <- combined %>%
  select(peak_position, danceability, energy, loudness, speechiness, 
         acousticness, instrumentalness, liveness, valence, tempo, explicit, 
         mode_name) %>% 
  na.omit()

# Run a linear model with all predictors
linear_model <- lm(peak_position ~ ., data = model_data)

# Run a random forest model with all predictors
rf_model <- randomForest(peak_position ~ ., data = model_data)

## TOPIC TWO

# Comparing model performance

# Predict song's peak position based on linear model
linear_prediction <- predict(linear_model, newdata = model_data)

# Predict song's peak position based on random forest model
rf_prediction <- predict(rf_model, newdata = model_data)

# Calculate RMSE for each model: square root of the mean squared error
linear_mse <- mean((model_data$peak_position - linear_prediction)^2)
linear_rmse <- sqrt(linear_mse)

rf_mse <- mean((model_data$peak_position - rf_prediction)^2)
rf_rmse <- sqrt(rf_mse)

cat("Linear model RMSE: ", linear_rmse, "\n")
cat("Random forest RMSE: ", rf_rmse)

## TOPIC THREE

# Splitting data into testing and training

# Create fold vector
fold <- rep_len(x = 1:5, length.out = nrow(model_data))

# One test/train
training <- model_data %>%
  filter(fold != 1)
testing <- model_data %>%
  filter(fold == 1)

# Use training data only to build & evaluate model

# Run a linear model with all predictors
linear_model <- lm(peak_position ~ ., data = training)

# Run a random forest model with all predictors
rf_model <- randomForest(peak_position ~ ., data = training)

# Predict song's peak position based on linear model for TESTING data
linear_prediction <- predict(linear_model, newdata = testing)

# Predict song's peak position based on random forest model for TESTING data
rf_prediction <- predict(rf_model, newdata = testing)

# Calculate RMSE for each model: square root of the mean squared error
linear_mse <- mean((testing$peak_position - linear_prediction)^2)
linear_rmse <- sqrt(linear_mse)

rf_mse <- mean((testing$peak_position - rf_prediction)^2)
rf_rmse <- sqrt(rf_mse)

# cat("Linear model RMSE: ", linear_rmse, "\n")
# cat("Random forest RMSE: ", rf_rmse)

# Note both got worse, but RF got considerably worse (almost double RMSE)

# Now need to do this for each of the folds
# Data frame to hold results
ml_results <- data.frame(fold = 1:5,
                         linear_rmse = NA,
                         rf_rmse = NA)
for (f in 1:5) {
  # One test/train
  training <- model_data %>%
    filter(fold != f)
  testing <- model_data %>%
    filter(fold == f)
  
  # Use training data only to build & evaluate model
  
  # Run a linear model with all predictors
  linear_model <- lm(peak_position ~ ., data = training)
  
  # Run a random forest model with all predictors
  rf_model <- randomForest(peak_position ~ ., data = training)
  
  # Predict song's peak position based on linear model for TESTING data
  linear_prediction <- predict(linear_model, newdata = testing)
  
  # Predict song's peak position based on random forest model for TESTING data
  rf_prediction <- predict(rf_model, newdata = testing)
  
  # Calculate RMSE for each model: square root of the mean squared error
  linear_mse <- mean((testing$peak_position - linear_prediction)^2)
  linear_rmse <- sqrt(linear_mse)
  
  rf_mse <- mean((testing$peak_position - rf_prediction)^2)
  rf_rmse <- sqrt(rf_mse)
  
  ml_results$linear_rmse[f] <- linear_rmse
  ml_results$rf_rmse[f] <- rf_rmse
}

ml_results

# Calculate means for each
linear_mean <- mean(ml_results$linear_rmse)
rf_mean <- mean(ml_results$rf_rmse)

cat("Linear model mean RMSE: ", linear_mean, "\n")
cat("Random forest mean RMSE: ", rf_mean, "\n")

# Make a plot?


```

## [TOPIC ONE]

Running the two models

Vanilla linear regression
Regression trees

***

## [TOPIC TWO]

Comparing the two models via RMSE (no training/testing split)

***

## [TOPIC THREE]

Training & testing split, a little bit why, mostly how
Comparing the two models via RMSE means

***

## [TOPIC FOUR]

Next steps

1. Variable (feature) selection
2. Poisson regression instead of vanilla linear regression
3. Poisson model for random forest

***

## Additional resources

+ An extremely useful resource is [An Introduction to Statistical Learning with Applications in R](https://arizona-primo.hosted.exlibrisgroup.com/permalink/f/evot53/01UA_ALMA51605092790003843), 
by Gareth James and colleagues. This book provides an accessible introduction 
to several machine learning approaches, including regression and classification
techniques, along with _a lot_ of worked examples and R code.
+ [resource two](url-two)
+ A [PDF version](https://jcoliver.github.io/learn-r/lesson-name.pdf) of this lesson

***

<a href="index.html">Back to learn-r main page</a>
  
Questions?  e-mail me at <a href="mailto:jcoliver@arizona.edu">jcoliver@arizona.edu</a>.