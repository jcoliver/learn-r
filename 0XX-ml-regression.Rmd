---
title: "Machine learning in R part 1: regression"
author: "Jeff Oliver"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: default
  pdf_document:
    latex_engine: xelatex
urlcolor: blue
---

An introduction to the concept of machine learning, with a worked application 
showing model comparison and evaluation of model performance.

#### Learning objectives
1. Explain the difference between machine learning and inferential statistics
2. Load tabular data into R
3. Build a predictive model and evaluate model performance
4. Split data into training and testing sets
5. Apply iteration to model evaluation and model selection

Machine learning has been all the rage, but there remains a lot of mystery 
about what machine learning really _is_. In this workshop, we will work through 
an example of how we use machine learning to make predictions and how to assess 
how good those predictions really are. The material is designed for people who 
have little to no experience with machine learning.

***

## Getting started

### Install additional R packages

There are _two_ additional R packages that will need to be installed:

+ dplyr
+ randomforest

To install these, run:

```{r install-libraries, eval = FALSE}
install.packages("dplyr")
install.packages("randomforest")
```

### Workspace organization

First we need to setup our development environment. Open RStudio and create a 
new project via:

+ File > New Project...
+ Select 'New Directory'
+ For the Project Type select 'New Project'
+ For Directory name, call it something like "ml-regression" (without the quotes)
+ For the subdirectory, select somewhere you will remember (like "My Documents" 
or "Desktop")

We need to create two folders: 'data' will store the data we will be analyzing, 
and 'output' will store the results of our analyses. In the RStudio console:

```{r workspace-setup, eval = FALSE}
dir.create(path = "data")
dir.create(path = "output")
```

It is good practice to keep input (i.e. the data) and output separate. 
Furthermore, any work that ends up in the **output** folder should be completely
disposable. That is, the combination of data and the code we write should allow 
us (or anyone else, for that matter) to reproduce any output.

### Example data

Explain where the data come from

save it in the 'data' folder that you created in the step above.

***

## Machine learning in a nutshell

In this lesson, we are going to run through an relatively minimal example of 
machine learning. The term "machine learning" gets thrown around a lot, often 
with little explanation, so we will start with a _very_ brief explanation. The 
big difference between "traditional" statistics and machine learning is the 
goal of each approach (aside: the modifier "traditional" is in no way meant to 
imply a lesser status or utility of statistics - I just could not come up with 
a better term). That is, the statistics we learned in class are generally 
focused on making inferences about how the world works, i.e. how does $X$ 
affect $Y$? In contrast, machine learning is less concerned with the details of 
how $X$ and $Y$ are related, but rather focused on using $X$ to make accurate 
predictions of $Y$. If we consider this in a linear regression framework, 
statistics cares about getting accurate values for the beta hats 
($\hat{\beta}$) on the right-hand side of the equation, while machine learning 
cares about being able to make accurate predictions for the Y hats ($\hat{Y}$) 
on the left-hand side of the equation:

$$
\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X_1 
$$

***

```{r dev-block}
# Place for experimentation; replace later
# Merging Taylor Swift data

library(dplyr)
library(randomForest)

# Chart data for Beyonce (& TS) available at 
# https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-09-29
# How do we get song characteristics? Might look at how it was done for TS:
# https://github.com/wjakethompson/taylor

# Song characteristics
# https://github.com/rfordatascience/tidytuesday/blob/master/data/2023/2023-10-17/taylor_all_songs.csv
# Billboard performance
# https://github.com/scharfsteina/BigDataFinalProject/blob/main/data/billboard.csv


## TOPIC FIVE (data wrangling)

songs <- read.csv(file = "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-10-17/taylor_all_songs.csv")
scores <- read.csv(file = "https://raw.githubusercontent.com/scharfsteina/BigDataFinalProject/main/data/billboard.csv")
colnames(scores)[1] <- "id"

# songs <- read.csv(file = "~/Desktop/taylor_all_songs.csv")
# scores <- read.csv(file = "~/Desktop/billboard.csv")

songs$track_name <- tolower(songs$track_name)

combined <- scores %>%
  left_join(songs, by = join_by(song == track_name))

# We want to predict peak_position
# Predictors (aka "features")
# danceability energy loudness speechiness acousticness instrumentalness liveness valence tempo explicit mode_name

model_data <- combined %>%
  select(peak_position, danceability, energy, loudness, speechiness, 
         acousticness, instrumentalness, liveness, valence, tempo, explicit, 
         mode_name) %>% 
  na.omit()

# Run a linear model with all predictors
linear_model <- lm(peak_position ~ ., data = model_data)

# Run a random forest model with all predictors
rf_model <- randomForest(peak_position ~ ., data = model_data)

## TOPIC TWO

# Comparing model performance

# Predict song's peak position based on linear model
linear_prediction <- predict(linear_model, newdata = model_data)

# Predict song's peak position based on random forest model
rf_prediction <- predict(rf_model, newdata = model_data)

# Calculate RMSE for each model: square root of the mean squared error
linear_mse <- mean((model_data$peak_position - linear_prediction)^2)
linear_rmse <- sqrt(linear_mse)

rf_mse <- mean((model_data$peak_position - rf_prediction)^2)
rf_rmse <- sqrt(rf_mse)

cat("Linear model RMSE: ", linear_rmse, "\n")
cat("Random forest RMSE: ", rf_rmse)

## TOPIC THREE

# Splitting data into testing and training

# Create fold vector
fold <- rep_len(x = 1:5, length.out = nrow(model_data))

# One test/train
training <- model_data %>%
  filter(fold != 1)
testing <- model_data %>%
  filter(fold == 1)

# Use training data only to build & evaluate model

# Run a linear model with all predictors
linear_model <- lm(peak_position ~ ., data = training)

# Run a random forest model with all predictors
rf_model <- randomForest(peak_position ~ ., data = training)

# Predict song's peak position based on linear model for TESTING data
linear_prediction <- predict(linear_model, newdata = testing)

# Predict song's peak position based on random forest model for TESTING data
rf_prediction <- predict(rf_model, newdata = testing)

# Calculate RMSE for each model: square root of the mean squared error
linear_mse <- mean((testing$peak_position - linear_prediction)^2)
linear_rmse <- sqrt(linear_mse)

rf_mse <- mean((testing$peak_position - rf_prediction)^2)
rf_rmse <- sqrt(rf_mse)

# cat("Linear model RMSE: ", linear_rmse, "\n")
# cat("Random forest RMSE: ", rf_rmse)

# Note both got worse, but RF got considerably worse (almost double RMSE)

# Now need to do this for each of the folds
# Data frame to hold results
ml_results <- data.frame(fold = 1:5,
                         linear_rmse = NA,
                         rf_rmse = NA)
for (f in 1:5) {
  # One test/train
  training <- model_data %>%
    filter(fold != f)
  testing <- model_data %>%
    filter(fold == f)
  
  # Use training data only to build & evaluate model
  
  # Run a linear model with all predictors
  linear_model <- lm(peak_position ~ ., data = training)
  
  # Run a random forest model with all predictors
  rf_model <- randomForest(peak_position ~ ., data = training)
  
  # Predict song's peak position based on linear model for TESTING data
  linear_prediction <- predict(linear_model, newdata = testing)
  
  # Predict song's peak position based on random forest model for TESTING data
  rf_prediction <- predict(rf_model, newdata = testing)
  
  # Calculate RMSE for each model: square root of the mean squared error
  linear_mse <- mean((testing$peak_position - linear_prediction)^2)
  linear_rmse <- sqrt(linear_mse)
  
  rf_mse <- mean((testing$peak_position - rf_prediction)^2)
  rf_rmse <- sqrt(rf_mse)
  
  ml_results$linear_rmse[f] <- linear_rmse
  ml_results$rf_rmse[f] <- rf_rmse
}

ml_results

# Calculate means for each
linear_mean <- mean(ml_results$linear_rmse)
rf_mean <- mean(ml_results$rf_rmse)

cat("Linear model mean RMSE: ", linear_mean, "\n")
cat("Random forest mean RMSE: ", rf_mean, "\n")

# Make a plot?


```

## [TOPIC ONE]

Running the two models

Vanilla linear regression
Regression trees

***

## [TOPIC TWO]

Comparing the two models via RMSE (no training/testing split)

***

## [TOPIC THREE]

Training & testing split, a little bit why, mostly how
Comparing the two models via RMSE means

***

## [TOPIC FOUR]

Next steps

1. Variable (feature) selection
2. Poisson regression instead of vanilla linear regression
3. Poisson model for random forest

***

## Bonus Track: Data Wrangling

The data we used for this lesson did not come in the form that we used them. 
That is, there were several steps that needed to happen _before_ we started 
using the data in the models. These steps were:

1. Downloading the original song data
2. Downloading the original score data for each song
3. Combining the two above data sources
4. Removing unnecessary columns from data
5. Removing any rows that are missing data

The section below illustrates these four steps. Note the cleaned data are all 
available (**TODO**: add a link to data), so you do not need to run the code below in order to do the machine 
learning parts of the lesson. But for those who are curious, the code below 
highlights that most machine learning applications require substantial curation 
of data sources before any models are built.

```{r data-download}
# Location of song data on Tidy Tuesday site 2023-10-17
song_url <- "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-10-17/taylor_all_songs.csv"
# Location of Billboard chart data
scores_url <- "https://raw.githubusercontent.com/scharfsteina/BigDataFinalProject/main/data/billboard.csv"

# Download both datasets
songs <- read.csv(file = song_url)
scores <- read.csv(file = scores_url)
```

We can look at the column names for both datasets as a reality check. We will 
need to join the data sets together, so they will need to have one column with 
the same information. The column _names_ do not necessarily have to match, but 
the data do. The `songs` data has the following columns:

```{r songs-columns}
colnames(songs)
```

Since we are doing an analysis on the level of the song (remember, we want to 
predict an individual song's highest spot on the Billboard chart), the fifth 
column, `r colnames(songs)[5]`, looks like the one we will use to join it with 
the `scores` data. Speaking of the `scores` data, it has the following columns:

```{r scores-columns}
colnames(scores)
```

There are two things of note:

1. There is no `track_name` column in the `scores` data, but there is a `song` 
column.
2. The first column is just called `X`, which is  not very informative.

Let us look at the first six rows of the `scores` data:

```{r scores-head}
head(scores)
```

So the `song` column looks like it has the information wee need to join with 
the `songs` data. The `X` column just looks like an identifier, so we can 
rename the column with a more informative name.

```{r rename-x}
# Rename first column of scores data from "X" to "id"
colnames(scores)[1] <- "id"
```

Before we join the data together it is worthwhile glancing at the values in the 
`track_name` column of the `songs` data. Here we will just look at the first 
ten values:

```{r songs-tracks}
songs$track_name[1:10]
```

And compare that to the first ten songs in the `scores` data:

```{r scores-songs}
scores$song[1:10]
```

Now the song names in the two datasets are in different orders, but that is not 
a concern, because we will use functions in R that know how to line the rows up 
correctly. However, note that the song names in the `songs` data have capital 
letters (it uses title case), while song names in the `scores` data are all 
lower case letters. If we leave the data like this, R will not be able to join 
the data together the right way. That is, if we tried to join the datasets 
together as is, R would think that "Shake It Off" and "shake it off" are 
different songs. _We_ know that these are the same song, but we need to change 
the data so R also sees them as the same song. The easiest way to do this is to 
change _all_ the letters in song names to lower case. It looks like the song 
names in the `scores` data are already lower case, but we can do the 
transformation on both datasets, just to be sure.

```{r songs-to-lower}
# Change song names to lower case
songs$track_name <- tolower(songs$track_name)
scores$song <- tolower(scores$song)
```

One final check and we can see that both datasets now have song names that are 
all lower case letters:

```{r songs-check}
songs$track_name[1:10]
```

```{r scores-check}
scores$song[1:10]
```

**TODO**: Need to deal with punctuation. Scores has "i knew you were trouble.", 
but songs has "i knew you were trouble".

```{r}
combined <- scores %>%
  left_join(songs, by = join_by(song == track_name))

# We want to predict peak_position
# Predictors (aka "features")
# danceability energy loudness speechiness acousticness instrumentalness liveness valence tempo explicit mode_name

model_data <- combined %>%
  select(peak_position, danceability, energy, loudness, speechiness, 
         acousticness, instrumentalness, liveness, valence, tempo, explicit, 
         mode_name) %>% 
  na.omit()



```

Data wrangling code (read from tidytuesday sources & reduction to only those 
variables of interest)

***

## Additional resources

+ An extremely useful resource is [An Introduction to Statistical Learning with Applications in R](https://arizona-primo.hosted.exlibrisgroup.com/permalink/f/evot53/01UA_ALMA51605092790003843), 
by Gareth James and colleagues. This book provides an accessible introduction 
to several machine learning approaches, including regression and classification
techniques, along with _a lot_ of worked examples and R code.
+ [resource two](url-two)
+ A [PDF version](https://jcoliver.github.io/learn-r/lesson-name.pdf) of this lesson

***

<a href="index.html">Back to learn-r main page</a>
  
Questions?  e-mail me at <a href="mailto:jcoliver@arizona.edu">jcoliver@arizona.edu</a>.