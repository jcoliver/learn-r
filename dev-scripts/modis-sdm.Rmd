---
title: "Land cover data for species distribution modeling"
author: "Jeff Oliver"
date: "2021-11-12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(package.startup.message = FALSE)
```

# Summary

An investigation into the use of MODIS land-cover data for species distribution 
models.

# Setup

## Dependencies

The work below uses additional package to download, visualize, and analyze the 
data.

```{r load-libraries, echo = FALSE}
suppressPackageStartupMessages({
  library(raster)      # reading and transforming geospatial data
  library(ggplot2)     # data visualization
  library(dplyr)       # data wrangling
  library(terra)       # dealing with hdf-formatted files
  library(MODIStsp)    # downloading land cover data
})
```

```{r load-libraries-display, eval = FALSE}
library(raster)      # reading and transforming geospatial data
library(ggplot2)     # data visualization
library(dplyr)       # data wrangling
library(terra)       # dealing with hdf-formatted files
library(MODIStsp)    # downloading land cover data
```

## Earthdata access

To access the data, you'll need to register with NASA's Earthdata system at 
[https://urs.earthdata.nasa.gov/users/new](https://urs.earthdata.nasa.gov/users/new). 
After registering, also be sure to navigate to your 
[profile page](https://urs.earthdata.nasa.gov/profile), click the on the 
Applications menu and the Authorized Apps submenu. Click the "Approve More 
Applications" button near the bottom of the page and click the "Authorize" 
button in the row labeled "LP DAAC Data Pool". Make a note of your username and 
password, as we will need those when downloading the data. Note the password 
is not super-securely stored by NASA, so don't use one that is especially 
important to you (I'm not going to lecture anyone about never re-using 
passwords - we all do it).

# Data retrieval

In this example, we use land cover data available from
[MODIS](https://modis.gsfc.nasa.gov/data/dataprod/). There are many means of 
categorizing land cover, and in this example we use the land cover scheme of 
the International Geosphere-Biosphere Programme (IGBP) that identifies 17 
classes of land cover. There are two options for retrieving this land cover 
data:

1. Direct download
2. Use MODIStsp package

## Direct download

The entirety of the land cover data from MODIS can be downloaded for years 
2010-2019 from [https://e4ftl01.cr.usgs.gov/MOTA/MCD12C1.006/](https://e4ftl01.cr.usgs.gov/MOTA/MCD12C1.006/). 
For this example, we will just use the 2019 data available at [https://e4ftl01.cr.usgs.gov/MOTA/MCD12C1.006/2019.01.01/](https://e4ftl01.cr.usgs.gov/MOTA/MCD12C1.006/2019.01.01/) 
by downloading the file MCD12C1.A2019001.006.2020220162300.hdf. The file is 
fairly large (1.2 GB), so it might take a few minutes to download. After it 
downloads, place this file in a "data" directory in your project. 

The downloaded file is global in extent and uses a bit of an old projection 
system. So the next bit of code limits the data to the area of interest and 
re-projects it into the commonly used WGS84 projection.

```{r hdf-investigation, results = "hide"}
# Location of hdf file with land cover data
hdf_file <- "data/MCD12C1.A2019001.006.2020220162300.hdf"

# Look to see what layers are in that hdf file
terra::describe(hdf_file, sds = TRUE)
```

The table output from `terra::describe()` shows us that we are interested in 
the first sub-dataset, which has the IGBP land-cover classification 
information. When we load in the data, we can pull in just that one sub-dataset 
of interest. We can then see what coordinate reference system (CRS) or 
projection is being used with `terra::crs()`:

```{r hdf-projection}
# Load in the first subdataset only, which has IGBP land cover type
r <- terra::rast(hdf_file, subds = 1)

# TODO: Would it be better instead to use the 17 layers with actual percentages
# rather than the single land cover type with the highest percentage?
# r <- terra::rast(hdf_file, subds = 3)

# What projection is being used?
terra::crs(r, describe = TRUE)
```

Well that's not the most useful information, but it turns out this vague 
projection is [nearly identical to a more common one, NAD27](https://community.esri.com/t5/data-management-questions/converting-from-clarke-1866-to-wgs-1984/td-p/255165). So we are going to tell R to go ahead and re-categorize the 
projection as NAD27.

```{r update-crs}
# Start by making it clear which EPSG:4008 projection this is (may not be 
# entirely necessary)
terra::crs(r) <- "epsg:4008"

# Set the projection to something we know (NAD27 is apparently close); the EPSG
# identifier for NAD27 is 4267
terra::crs(r) <- "epsg:4267"
```

Now R considers these data as NAD27, but we need to re-project the data into 
yet _another_ coordinate system (well, we may not have to, but NAD27 is a bit 
out of date). While we are at it, we can crop the data down to a smaller 
geographic area so it doesn't take up as much memory.

```{r re-project}
# Crop to western US + Mexico
crop_to <- raster::extent(x = c(-120, -104, 22, 49))
r <- raster::crop(x = r, y = crop_to)

# Convert to a RasterLayer, which is easier to work with
r_nad27 <- raster::raster(r)

# Rename what the data are (otherwise it just uses the file name)
names(r_nad27) <- "IGBP_Land_Cover_Type"

# Convert the projection to good old WGS84
# TODO: Sometimes this throws an idiosyncratic error:
#     Error in x$.self$finalize() : attempt to apply non-function
# if that happens, just try running it again
r_wgs84 <- raster::projectRaster(from = r_nad27, 
                                 method = "ngb", # For categorical data
                                 crs = sp::CRS(projargs = "+init=epsg:4326"))

# Crop this to Sonoran desert dimensions
sonoran <- raster::extent(x = c(-114, -109, 26, 35))
r_wgs84 <- raster::crop(x = r_wgs84, y = sonoran)

# Finally, save to a file
raster::writeRaster(x = r_wgs84,
                    filename = "data/Sonoran_land_cover.tif",
                    overwrite = TRUE)
```


If you download the data this way, you can skip the next section and head 
straight to [Data quality check](#data-quality-check).

## Use MODIStsp package

The R package MODIStsp provides a way to download portions of the land cover 
data, instead of the full, global coverage. In my experience, I encountered 
quite a few problems with accessing data from servers, so I would only 
recommend this approach if the direct download option above doesn't work.

If you want to get details about data availability, you can run:

```{r modis-check}
MODIStsp::MODIStsp_get_prodlayers("MCD12Q1")
```

It's that first band that we want ("LC1", or "Land Cover Type 1 (IGBP)*"). To 
download the data, we use the `MODIStsp()` function from the MODIStsp package. 
Much of the code for downloading and visualizing these data from the example at 
[https://rspatialdata.github.io/land_cover.html](https://rspatialdata.github.io/land_cover.html).
Here's where you'll need to add your usename and password that you used to 
register for NASA's Earthdata site. In the example below, I stored them in a 
pair of plain text files called earthdata-username.txt and 
earthdata-password.txt. I then read in the values and stored them in variables 
`username` and `password` respectively.

```{r load-credentials, eval = FALSE}
username <- scan(file = "../keys/earthdata-username.txt", what = character())
password <- scan(file = "../keys/earthdata-password.txt", what = character())
```

You could also do this in the console if you don't want to store your username 
and/or password in files (replacing `<INSERT USERNAME HERE>` and 
`<INSERT PASSWORD HERE>` with your actual username and password):

```{r assign-credentials, eval = FALSE}
# Run in the console:
username <- "<INSERT USERNAME HERE>"
password <- "<INSERT PASSWORD HERE>"
```

We will download land cover data for a box that roughly encapsulates the 
Sonoran desert. The code below downloads the data to a folder called "data", so 
be sure that you have created such a folder first (i.e. `dir.create("data")`).

```{r download-sonoran-raster, eval = FALSE}
# Boundaries of interest (left, bottom, right, top); here we use very rough 
# coordinates for the Sonoran desert
bounds <- c(-114, 26, -109, 35)

# Need to specify which projection to use; we do this with the WKT (Well Known 
# Text) string for the WGS 84 projection. A terrifying thing, but it works. 
# Using paste0 so it all fits in this document
wkt_proj <- paste0('GEOGCS["WGS 84",DATUM["WGS_1984",SPHEROID["WGS84",6378137,',
                   '298.257223563,AUTHORITY["EPSG","7030"]],AUTHORITY["EPSG",',
                   '"6326"]],PRIMEM["Greenwich",0,AUTHORITY["EPSG","8901"]],',
                   'UNIT["degree",0.01745329251994328,AUTHORITY["EPSG","9122"]',
                   '],AUTHORITY["EPSG","4326"]]')

# Query server for data of interest
MODIStsp::MODIStsp(gui = FALSE,
                   out_folder = "data",
                   out_folder_mod = "data",
                   selprod = "LandCover_Type_Yearly_500m (MCD12Q1)",
                   bandsel = "LC1", 
                   start_date = "2019.01.01", 
                   end_date = "2019.12.31", 
                   user = username,
                   password = password,
                   verbose = FALSE, # set to true if you want to see details
                   reprocess = TRUE,
                   spatmeth = "bbox",
                   bbox = bounds,
                   out_projsel = "User Defined",
                   output_proj = wkt_proj,
                   out_format = "GTiff")
```

The servers where these data live are notorious for timing out. Sometimes it 
takes a few times of running the above code for it to actually work. If you see 
multiple lines of 

`Error in curl::curl_fetch_memory(url, handle = handle): Timeout was reached`

try running the `MODIStsp()` command again.

# Data quality check

After downloading the data, we can do a reality check and plot the land cover 
data. We will need to read the data into memory and transform it into a data 
frame for easier plotting with ggplot. In the code below, we'll assume you 
used the first option ([Direct download](#direct-download)).

```{r plot-raster}
# Load in raster from file
LC_raster <- raster::raster(x = "data/Sonoran_land_cover.tif")

# Convert data to categorical (R thinks it is continuous)
# LC_raster <- raster::as.factor(x = LC_raster)

# Convert to a data frame for plotting and make sure we set the land cover 
# types to a 17-level factor (want to do this because our subset of data does 
# not include all land cover types, but we need the factor to have all 17 
# levels so we can assign land cover types appropriately)
LC_df <- as.data.frame(LC_raster, xy = TRUE, na.rm = TRUE) %>%
  rename(lc_type = Sonoran_land_cover) %>%
  mutate(lc_type = factor(lc_type, levels = 0:16))

# Re-level data
levels(LC_df$lc_type) <- c("Water Bodies",
                           "Evergreen needleleaf forests",
                           "Evergreen broadleaf forests",
                           "Deciduous needleleaf forests",
                           "Deciduous broadleaf forests",
                           "Mixed forests",
                           "Closed shrublands",
                           "Open shrublands",
                           "Woody savannas",
                           "Savannas",
                           "Grasslands",
                           "Permanent wetlands",
                           "Croplands",
                           "Urban and built-up lands",
                           "Cropland/natural vegetation mosaics",
                           "Snow and ice",
                           "Barren")

# Visualize using ggplot2
lc_plot <- ggplot(data = LC_df, 
                  mapping = aes(x = x, y = y, fill = lc_type)) + 
  geom_raster() +
  scale_fill_viridis_d(name = "Land Cover Type") + # the color scheme
  labs(subtitle = "Land cover 2019",
       x = "Longitude",
       y = "Latitude") +
  theme_minimal() +
  theme(legend.text = element_text(size = 8))
print(lc_plot)
```

# Species distribution modeling

We will use a suite of libraries for the SDM; they might have been loaded 
already, but I put these `library()` calls here just to be sure.

```{r load-sdm-libraries, echo = FALSE}
suppressPackageStartupMessages({
# Load additional packages
library(sp)       # general mapping and raster utility
library(raster)   # for dealing with land cover data
library(maptools) # mapping distributions and model predictions
library(dplyr)    # data wrangling
library(dismo)    # background (pseudo-absence) point generation
})
```

```{r load-sdm-libraries-display, eval = FALSE}
library(sp)       # general mapping and raster utility
library(raster)   # for dealing with land cover data
library(maptools) # mapping distributions and model predictions
library(dplyr)    # data wrangling
library(dismo)    # background (pseudo-absence) point generation
```

## Data preparation

We will use observations of the saguaro cactus, _Carnegiea gigantea_, for this 
example. To run the SDM, we will need both presence and absence data, as well 
as the land cover data.

### Presence data

You can download your own set of data from GBIF, or use the code below
to read them in from a URL (these are the same data as used in the SDM lesson 
at [https://jcoliver.github.io/learn-r/011-species-distribution-models.html](https://jcoliver.github.io/learn-r/011-species-distribution-models.html)).

```{r load-saguaro}
url <- "https://raw.githubusercontent.com/jcoliver/learn-r/gh-pages/data/Carnegiea-gigantea-GBIF.csv"
obs_data <- read.csv(file = url)
head(obs_data)
```

The data include more columns than we need, and some of the rows lack 
geographical coordinate data, so we'll just keep the latitude and longitude 
columns and remove rows missing latitude or longitude values.

```{r clean-saguaro}
# Drop any rows with NAs in latitude
obs_data <- obs_data[!is.na(obs_data$latitude), ]

# Only pull out those columns of interest and in the order we want them (i.e. 
# longitude as the first column, latitude as the second column)
obs_data <- obs_data[, c("longitude", "latitude")]
head(obs_data)
```

### Preparing land cover data

We load in the land cover data that we saved previously. We need to make sure 
the land cover data are in a format we can use. Since the land cover data is 
categorical, we need to convert the data from a integer to a factor.

```{r prep-land-cover}
lc_data <- raster::raster(x = "data/Sonoran_land_cover.tif")
lc_data <- raster::as.factor(lc_data)
names(lc_data) <- "lc_type" # Name the layer for ease of use later
```

### (Pseudo)absence data

To simulate absence data, we will randomly sample points from a bounding box
defined by the observations of saguaros. To do this, we find the minimum and 
maximum of longitude and latitude and store that information in an `extent` 
object. For the bounding box, we use `ceiling()` and `floor()` to round down or 
up, respectively, to the nearest degree.

```{r create-background}
# Determine geographic extent of our data
max_lat = ceiling(max(obs_data$latitude))
min_lat = floor(min(obs_data$latitude))
max_lon = ceiling(max(obs_data$longitude))
min_lon = floor(min(obs_data$longitude))
geographic_extent <- raster::extent(x = c(min_lon, max_lon, min_lat, max_lat))

# Using the resolution of our predictor data (i.e. the land cover data), we 
# randomly sample the same number of background points as we have observations
background <- dismo::randomPoints(mask = lc_data,     # Provides resolution of sampling points
                                  n = nrow(obs_data),      # Number of random points
                                  ext = geographic_extent, # Spatially restricts sampling
                                  extf = 1.25)             # Expands sampling a little bit
# Rename column names of background to longitude and latitude (from x and y)
colnames(background) <- c("longitude", "latitude")
```

## Training & testing the model

For this SDM, we will use a generalized linear model, instead of the `bioclim`
approach designed for the bioclimate variables. To do this, we need to have our 
data in a particular shape. That is, we are going to create a data frame that 
includes all the observed data, the background (pseudo-absence) data, and the 
landcover data. The data frame will also include a column called `fold` which 
we use to determine if a row is part of the training data or part of the 
testing data. For training and test the model, we split the data into a 
training set, which we used to build the model, and a testing set, which we use 
to evaluate the model. We will use a 80:20 training:testing split, which is 
common for SDM applications.

The table will look like:

| longitude | latitude | pa | fold | lc_type |
|:----------|:---------|:---|:-----|:--------|
| -110.90   | 32.34    |  1 |   2  |      1  |
| -110.72   | 30.29    |  1 |   3  |      6  |
| -112.38   | 30.18    |  0 |   1  |     10  |

Where 1 indicates "presence" and 0 indicates "absence" in the `pa` column. The 
data for `pa = 1` rows will come from `obs_data` and the data for `pa = 0` rows 
will come from `background`.

```{r shape-data}
# Use the observed points to pull out relevant predictor values
predictors_presence <- raster::extract(x = lc_data, y = obs_data)
predictors_absence <- raster::extract(x = lc_data, y = background)

# Make a vector of appropriate length with 0/1 values for 
# (pseudo)absence/presence
pa_data <- c(rep(x = 1, times = nrow(obs_data)), 
             rep(x = 0, times = nrow(background)))
  
# Set random number generator for repeatability of testing/training split
set.seed(20211119)

# Create a vector of folds for easier splitting into testing/training
testing_group <- 1
num_folds <- 5 # for 20/80 split
fold <- c(rep(x = 1:num_folds, length.out = nrow(obs_data)),
          rep(x = 1:num_folds, length.out = nrow(background)))
  
# Combine our presence / absence and fold vectors with environmental data we 
# extracted
full_data <- data.frame(cbind(pa = pa_data,
                              fold = fold,
                              lc_type = c(predictors_presence, 
                                          predictors_absence)))

# And we want to make sure the land cover data are treated as factors 
# (categorical data) instead of numbers (continuous data).
full_data$lc_type <- as.factor(x = full_data$lc_type)

# Create separate data frames for testing and training presence data
presence_train <- full_data %>%
  filter(pa == 1) %>%
  filter(fold != testing_group)
presence_test <- full_data %>%
  filter(pa == 1) %>%
  filter(fold == testing_group)
# Create separate data frames for testing and training (pseudo)absence data
absence_train <- full_data %>%
  filter(pa == 0) %>%
  filter(fold != testing_group)
absence_test <- full_data %>%
  filter(pa == 0) %>%
  filter(fold == testing_group)

# Add presence and pseudoabsence training data into single data frame
sdmtrain <- rbind(presence_train, absence_train)
sdmtest <- rbind(presence_test, absence_test)
```

We are _finally_ ready to run the species distribution model!

```{r run-sdm}
# Run generalized linear model on training data
sdm_model <- stats::glm(pa ~ lc_type, 
                        data = sdmtrain, 
                        family = binomial(link = "logit"))

# Evaluate model performance with testing data
sdm_eval <- dismo::evaluate(p = presence_test, 
                            a = absence_test, 
                            model = sdm_model)

# Calculate threshold so we can make a presence/absence map later
pres_threshold <- dismo::threshold(x = sdm_eval, 
                                   stat = "spec_sens")
```

## Visualizing the results

Now that the model is built, we can generate a pair of maps:

1. Showing probability of presence
2. A threshold map, showing the distribution where the organism of interest is 
predicted to be present

We start with a map showing raw predicted probabilities from the model.

```{r visualize-sdm-probs}
# We are going to extend the predictions a bit beyond the observed data; here 
# we extend the area for predictions by 50%
pred_extf <- 1.5

# Based on the sdm model and the original land cover data, we can predict the 
# probabilities of presence
probs <- predict(lc_data,
                 sdm_model,
                 ext = geographic_extent * pred_extf)

# Because of the model we used (generalized linear model), it is possible to 
# get predictions of negative probabilities (!), we will go ahead and set any 
# negative probabilities to 0
probs[probs < 0] <- 0
# ...as well as probabilities above 1; we'll reset those to 1
probs[probs > 1] <- 1

# Plot base map
data(wrld_simpl)
plot(wrld_simpl, 
     xlim = c(min(background[, "longitude"]), max(background[, "longitude"])),
     ylim = c(min(background[, "latitude"]), max(background[, "latitude"])),
     axes = TRUE, 
     col = "grey95")

# We add the probabilities to the map, although they are tough to interpret
plot(probs, 
     add = TRUE)

# And add those observations
points(x = obs_data$longitude, 
       y = obs_data$latitude, 
       col = "black",
       pch = "+", 
       cex = 0.6)

# Redraw the borders
plot(wrld_simpl, add = TRUE, border = "grey5")
```

That map is a little funky, and we could clean it up, but for now we are going 
to move on to use the model evaluation results to create a presence/absence map.

```{r visualize-sdm-pa}
# Plot base map
data(wrld_simpl)
plot(wrld_simpl, 
     xlim = c(min(background[, "longitude"]), max(background[, "longitude"])),
     ylim = c(min(background[, "latitude"]), max(background[, "latitude"])),
     axes = TRUE, 
     col = "grey95")

# Only plot areas where probability of occurrence is greater than the threshold
plot(probs > pres_threshold, 
     add = TRUE, 
     legend = FALSE, 
     col = c(NA, "olivedrab"))

# And add those observations
points(x = obs_data$longitude, 
       y = obs_data$latitude, 
       col = "black",
       pch = "+", 
       cex = 0.75)

# Redraw those country borders
plot(wrld_simpl, add = TRUE, border = "grey5")
```

The green in the map show the areas where the saguaro is predicted to be 
present based on the SDM model.

_Closing note_: In the example above, we use land cover as a categorical 
variable. That is, any particular site is categorized as one, and only one land 
cover type. Even if the area of interest was measured as 51% open shrublands 
and 49% grasslands, it was categorized as open shrublands. The land cover data 
from MODIS actually includes percentages, and a more nuanced approach would be 
to use those percentages as continuous-valued predictors, rather than the 
categorical approach described here. That would take a different series of data 
wrangling steps, but I'm pretty sure it's possible.