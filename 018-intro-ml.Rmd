---
title: "A taste of machine learning in R"
author: "Jeff Oliver"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document: default
  pdf_document:
    latex_engine: xelatex
urlcolor: blue
---

[INTRODUCTORY SENTENCE]

#### Learning objectives
1. Explain the difference between regression and classification problems
2. Train an artificial neural network **on what?**
3. Evaluate performance of two machine learning classification approaches
4. Explain the difference between supervised and unsupervised learning
5. Describe how biases can influence machine learning inferences

## [DESCRIPTION OR MOTIVATION; 2-4 sentences that would be used for an announcement]

***

## Getting started

+ Library installation (if necessary)
+ Project setup
+ Download data (if necessary)

***

Need a little preamble about machine learning being a huge field, this workshop 
won't make you a data scientist, and resources for learning more are at the end
of the lesson.

## [TOPIC ONE]

+ Classification vs. regression
+ Training a network for regression
    + Training vs. testing data

Want to set a seed to ensure same results?

Maybe use ggplot2's diamond dataset?

lm(price ~ cut + color + x + y + z, data = diamonds)

Diamonds really are a good dataset

Could also use something from Tidy Tuesday
Board Games (Predict rating?)
https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-03-12
Cricket matches (Predict win?)
https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-11-30/readme.md
Yarn (Predict rating?)
https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-10-11


Could do a simple linear regression then random forests / regression tree

```{r}
library(randomForest)
library(ggplot2)
library(tree)
dia_small <- diamonds[sample(x = 1:nrow(diamonds), size = 10000), ]
Sys.time()
# 33 seconds for ntree default (500)
# 6 seconds for ntree = 100
di_rf_3 <- randomForest(price ~ cut + carat + color + x + y + z, 
                        data = dia_small,
                        mtry = 3,
                        importance = TRUE,
                        ntree = 100)
Sys.time()
# 51 seconds for ntree default (500)
# 10 seconds for ntree = 100
di_rf_6 <- randomForest(price ~ cut + carat + color + x + y + z, 
                        data = dia_small,
                        mtry = 6,
                        importance = TRUE,
                        ntree = 100)
Sys.time()

# Want to plot variable importance, or at least look at it
importance(di_rf_3)

di_rt <- tree(price ~ cut + carat + color + x + y + z,
              data = dia_small)
summary(di_rt)
plot(di_rt)
text(di_rt, pretty = 0)

mtcars_rf_6 <- randomForest(mpg ~ cyl + disp + hp + drat + wt + gear, 
                          data = mtcars,
                          mtry = 3,
                          importance = TRUE)
Sys.time()
```

Can compare models with MSE, mean((predicted - observed)^2)

***

## [TOPIC TWO]

+ Classification two ways
+ Model evaluation

neuralnet library for classification problems

Use palmer penguins for classification, N = 344

***

## [TOPIC THREE]

+ Supervised vs. unsupervised (just a quick comparison)
+ One way bias affects results

Maybe do an unsupervised clustering on bigfoot data
https://github.com/rfordatascience/tidytuesday/tree/master/data/2022/2022-09-13


***

## Additional resources

+ [resource one](url-one) **Introduction to Statistical Learning**
+ [resource two](url-two) UITS ML on the HPC: https://public.confluence.arizona.edu/display/UAHPC/Machine+Learning+on+HPC
+ A [PDF version](https://jcoliver.github.io/learn-r/018-intro-ml.pdf) of this lesson

***

<a href="index.html">Back to learn-r main page</a>
  
Questions?  e-mail me at <a href="mailto:jcoliver@arizona.edu">jcoliver@arizona.edu</a>.